{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section notes by Wendy Valencia-Montoya (TF 2020),  adapted from Angel Tang (TF 2019), Jung-Eun Shin (TF 2018) and William Mallard (TF 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Non-negative matrix factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) (NMF)__ is a [matrix decomposition](https://en.wikipedia.org/wiki/Matrix_decomposition) method, where we constraint the matrices to be non-negative. This technique basically splits a matrix __V__ into two smaller __W__ and __H__ [non-negative matrices](https://en.wikipedia.org/wiki/Nonnegative_matrix) whose [product](https://en.wikipedia.org/wiki/Matrix_multiplication) is approximately equal to the original matrix.\n",
    "\n",
    "In a deep sense, we are actually __[compressing](https://en.wikipedia.org/wiki/Data_compression)__ our data through a rank factorization. We want to reduce the original dimensions of our matrix of observations __V__ of size *N×M* by decomposing it into two matrices __W__ and __H__ of size *N×R* and *R×M*. Since __W__ × __H__ only approximates __V__, this is actually __[lossy compression](https://en.wikipedia.org/wiki/Lossy_compression)__.\n",
    "\n",
    "Each column in __W__ is a basis element, which means some component that crops up again and again in all of the original *M* data points. These components are the fundamental building blocks we can use to reconstruct an approximation to all the observed data points. On the other hand, each column in __H__ gives you the coordinates of a data point, in the basis of __H__, i.e., it tells us how to reconstruct an approximation to the original data point from a linear combination of the building blocks in __W__. \n",
    "\n",
    "In other words, we can interpret __V__ to be a weighted sum of some components, where each column in __W__ is a component, and each column in __H__ contains the weights of each component. Therefore, we can think of NMF as descompossing each data point into an overlay of certain components.\n",
    "\n",
    "_Why do we want to do this?_ The idea is that there is some [rule-driven process generating our data](https://en.wikipedia.org/wiki/Generative_model); we have taken a number of noisy measurements of the system's output, and we would like to discover the underlying structure of our data -- and ultimately infer the specific rules of the generative model.  \n",
    "\n",
    "NMF has become so popular to infer underlying hidden variables mainly because of its ability to automatically extract sparse and easily interpretable factors. Indeed, NMF has been applied to diverse fields such as: astronomy,computer vision, document clustering, missing data imputation, chemometrics, audio signal processing, recommender systems, population genetics, and RNA expression data. For this section, we are about to go through an example of an application of NMF in population genetics to estimate admixture coefficients of different populations or species that may have interbred in the past, or that are subject to ongoing hybridization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are cloud forest Andean beetles a single population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Climatic fluctuations in the mountains can result in up or downslope shifts of entire ecosystems. A clever way of inferring the history of these shifts is by studying genetic patterns of current species that are tightly associated with a particular kind of ecosystem such a rainforest, cloud forest, or alpine tundra along an altitudinal gradient. Field biologists from a beetle genetic lab set up to investigate ecosystem shifts of the cloud forest of Andean mountains. One hypothesis is that a species of cloud forest beetle forms a single population, consistent with long-term altitudinal stability of this ecosystem. The competing model is that the beetle species consists of several populations despite there are none obvious geographic barriers. The latter hypothesis is based on their belief that the different populations formed in isolation in the mountaintops of different peaks. If the cloud forest just recently shifted downslope, it may not have been enough time to erode the genetic differentiation between populations, although hybridization may be expected.\n",
    "\n",
    "The beetle lab has genotyped 300 loci for 120 individuals and summarized the genetic variation by counting the number of sites with derived alleles per locus. Their data is a simple whitespace-delimited table, available here. \n",
    "\n",
    "Basically, the goal is to find the number of populations (the hidden parameter) that resulted in the observed data (genotypes). Additionally, you want to find potential hybrid individuals in case there is more than one population as a signature of ongoing genetic exchange. You have taken MCB112 and distinctly remember you can approach this problem through non-negative matrix factorization since you aim to find hidden components (populations) that group your individuals, but also admixture coefficients to identify hybrid individuals (weights of the different components). \n",
    "\n",
    "In summary, you have a matrix __V__ of observed genetic data (*NXM*), with *N* loci, across *M* individuals, and you want to decompose these derived allele counts in two matrices __W__ (*NxR*) and __H__ (*RxM*).\n",
    "\n",
    "If we succeed, then:\n",
    "\n",
    " - R is the number of populations\n",
    " - __W__ tells us the frequencies of derived alleles on the different populations.\n",
    " - __H__ tells us how much each of those populations contributes to the genetic makeup of an individual (admixture coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages we need to import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import non_negative_factorization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=300 #Number of loci genotyped\n",
    "M=120 #Number of individuals\n",
    "filename='w10-section-data.txt' #Data set\n",
    "V_obs=np.ones((N,M)) #Empty matrix to read the data\n",
    "loci_names=[]\n",
    "with open(filename, 'rt') as fd:\n",
    "    for n,line in enumerate(fd):\n",
    "        fields = line.split()\n",
    "        loci_names.append(fields[0])\n",
    "        V_obs[n] = [int(i) for i in fields[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** For the pset you have to implement the actual algorithms for NMF, but in the mean time, for the section and for getting familiar with the problems you can solve with NMF, we can use the awesome module scikit learn. \n",
    "\n",
    "We have to pass to the `non_negative_factorization` function the matrix of observed data __V_obs__, but we also have to decide the number of components, that is the *rank* for the factorization. In general, it is hard to know how to choose the factorization rank r. Some approaches include trial and error, estimation using the dacay in likelihood, and insights from experts. Here and in the pset, we can run the optimization with several n_components, and observed the change in log-likelihood. Therefore, it is handy to have a separate function to calculate tthe log likelihood (Here in pseudo-code).\n",
    "\n",
    "For implementing this function, we ignore the factorial term since it's independent of $W$ and $H$.  In matrix operation notation, the log likelihood is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log{P(V\\vert W, H)} \\sim \\sum_{\\mu, i} V\\log{(\\lambda)}-\\lambda,\n",
    "\\end{equation*}\n",
    "\n",
    "where the multiplication, log, and subtraction are elementwise operations, and the sum is over all elements in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(V, C, W, H):\n",
    "    \"\"\"input\n",
    "            V: observed counts matrix (NxM)\n",
    "            C: total number of counts per experiment (Rx1)\n",
    "            W: current weights matrix (NxR)\n",
    "            H: current mixture coefficients matrix (RxM)\n",
    "        output\n",
    "            ll: total log likelihood of data given model\n",
    "    \"\"\"\n",
    "    #Calculate expected counts V_exp from C, W, and H\n",
    "    \n",
    "    #V_exp=\n",
    "    \n",
    "    #Calculate loglikelihood using observed counts and expected counts\n",
    "    \n",
    "    #ll =\n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have a function to calculate the likelihood, you can run the NMF function with several numbers of components and evaluate the likelihood. Here, only the best number of populations (R=3) is included, and you can arrive at the same conclusion after implementing the algorithms for NMF and the likelihood function :)\n",
    "\n",
    "Besides, note that we are using the multiplicative update to resemble closer (solver = \"mu\")  Lee and Seung (1999) formulation. Also, we use \"Kullback-Leibler\" given that we have discrete counts, which is better suited than the Frobenius norm that assumes Gaussian noise. Check the function __[documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.non_negative_factorization.html)__ for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wendyvalencia/opt/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/nmf.py:1069: ConvergenceWarning: Maximum number of iteration 300 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "W, H, n_iter = non_negative_factorization(V_obs, beta_loss= 'kullback-leibler', n_components=3,init='random', solver = \"mu\",tol=0.000001, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may try a handful of numbers of components, and you can decide on what is a good number of components by evaluating the likelihood, using different values for R.\n",
    "\n",
    "A common plot of the admixture proportions portrays individuals in the X-axis and the percentage of the genomes coming from different populations. A widely used practice is to sort the array by population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_array_pop(V_array, n_comp):\n",
    "    for i in range(n_comp):\n",
    "        if i == 0:\n",
    "            pop1= V_array[:,np.argmax(V_array, axis =0) == i]\n",
    "            pop1_sort = pop1[:,pop1[i].argsort()[::-1]]\n",
    "        elif i == 1:\n",
    "            popn = V_array[:,np.argmax(V_array, axis =0) == i]\n",
    "            popn_sort = popn[:,popn[i].argsort()]\n",
    "            sorted_array = np.concatenate((pop1_sort,popn_sort), axis =1)\n",
    "        else:\n",
    "            popn = V_array[:,np.argmax(V_array, axis =0) == i]\n",
    "            popn_sort = popn[:,popn[i].argsort()]\n",
    "            sorted_array = np.concatenate((sorted_array,popn_sort), axis =1)\n",
    "    \n",
    "    return sorted_array    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should renormalize H, since unlike Sean in class, who implemented a smart trick to write H as probabilities and keep the counts separated, the NMF from sklearn function is directly optimizing the counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_nor = H / H.sum(axis=0)\n",
    "H_sorted = sort_array_pop(H_nor,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a function to visualize the genetic composition of each individual (i.e., the proportion of the genome of a single sampled beetle that comes from the three different inferred populations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_amixturelike(array_H):\n",
    "    sort_array_fin = array_H\n",
    "    r = [i for i in range(len(sort_array_fin[0]))]\n",
    "    raw_data = {'bluebars': list(100*(sort_array_fin[0,:])), 'redbars': list(100*(sort_array_fin[1,:])),'yellowbars': list(100*(sort_array_fin[2,:]))}\n",
    "    df = pd.DataFrame(raw_data)\n",
    "     \n",
    "    # From raw value to percentage\n",
    "    totals = [i+j+k for i,j,k in zip(df['bluebars'], df['redbars'], df['yellowbars'])]\n",
    "    bluebars = [i / j * 100 for i,j in zip(df['bluebars'], totals)]\n",
    "    redbars = [i / j * 100 for i,j in zip(df['redbars'], totals)]\n",
    "    yellowbars = [i / j * 100 for i,j in zip(df['yellowbars'], totals)]\n",
    "     \n",
    "    # plot\n",
    "    barWidth = 1\n",
    "    #names = ('A','B','C','D','E')\n",
    "    f, ax = plt.subplots(figsize=(20,5))\n",
    "    # Create green Bars\n",
    "    plt.bar(r, bluebars, color='lightskyblue', edgecolor='white', width=barWidth)\n",
    "    # Create orange Bars\n",
    "    plt.bar(r, redbars, bottom=bluebars, color='lightcoral', edgecolor='white', width=barWidth)\n",
    "    # Create blue Bars\n",
    "    plt.bar(r, yellowbars, bottom=[i+j for i,j in zip(bluebars, redbars)], color='khaki', edgecolor='white', width=barWidth)\n",
    "     \n",
    "    # Custom x axis\n",
    "    #plt.xticks(r, names)\n",
    "    plt.xlabel(\"Individual\")\n",
    "     \n",
    "    # Show graphic\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_amixturelike(H_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the plot, we can more easily appreciate that there are at least three different populations and several admixed individuals, lending support to the hypothesis of ancestral isolation between different populations of the cloud forest mountain beetle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly generate a __W__ matrix with _N_ rows and _R_ columns, such that each column sum up to one. The __H__ matrix can be generated similarly. May be useful when simulating your data and initialization for NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "N, M, R = 100, 50, 5 # N genes, M experiments, R modules\n",
    "W = np.random.dirichlet(np.ones(N), size=R).T # you can also use np.apply_along_axis as will be explained later\n",
    "print(W.shape)\n",
    "print(sum(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.random.dirichlet` generates a list of _n_ random numbers that sum to 1. Alpha is the typical Dirichlet parameter vector; lower values of alpha generate a sparser vector. This might come in handy when generating your __W__ matrix for your homework because a single gene modeule is likely to have a sparse relative expression vector. An illustration is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alphas = [10, 1, 0.5, 0.1]\n",
    "n = 100\n",
    "figs,axs = plt.subplots(1, 4, figsize=(18,3))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    axs[i].hist(np.random.dirichlet(alpha*np.ones(n)), bins=20, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add Poisson noise to a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original matrix is:\n",
      " [[828756.149 303564.694 180421.331 644968.406 591079.601]\n",
      " [771165.821 643607.776 331841.708  32676.559 814606.385]\n",
      " [378855.821 457563.917 153864.787 197833.729 777938.529]\n",
      " [403728.92  406048.646 429457.828 749506.651 388460.742]\n",
      " [662947.66   34275.355 570499.521 983140.53  738613.637]\n",
      " [380135.528 180238.314 814467.131 510705.973 409860.95 ]\n",
      " [953381.103 995632.591 939113.514 156562.724  25966.157]\n",
      " [119870.184 163262.768 765322.656 363544.147 831993.004]\n",
      " [151570.158  70383.063 699119.368 618149.925 512896.284]\n",
      " [ 18820.009 665923.425 265534.974 219362.422 215328.595]]\n"
     ]
    }
   ],
   "source": [
    "N, M = 10, 5\n",
    "L = 1e6 * np.random.random((N, M))\n",
    "\n",
    "print(\"The original matrix is:\\n\", np.round(L, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix after adding Poisson noise:\n",
      " [[829523 303630 180449 644121 590969]\n",
      " [770395 642546 330777  32474 814773]\n",
      " [379032 456548 153790 197722 776908]\n",
      " [403004 406241 429905 749256 389108]\n",
      " [661787  34442 570162 983121 740170]\n",
      " [381387 180107 813882 511232 410261]\n",
      " [954852 994043 939394 156152  26046]\n",
      " [119682 162260 766397 362810 831072]\n",
      " [150938  70413 698495 618950 511228]\n",
      " [ 18613 666378 265888 219690 215044]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The matrix after adding Poisson noise:\\n\", np.random.poisson(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "\n",
    "Python has a dedicated single-character [infix](https://en.wikipedia.org/wiki/Infix_notation) math operator specifically for the purpose of matrix multiplication: __the `@` symbol__. This [was added](https://www.python.org/dev/peps/pep-0465/) to Python3 to eliminate ambiguity about what `*` means in the context of two matrices.\n",
    "\n",
    "The `@` operator performs [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) on two matrices of compatible dimensions. eg, if `A` is an N×R matrix and `B` is an R×M matrix, then `A @ B` will give an N×M matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `*` operator performs [element-wise multiplication](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29) on two matrices of the same dimensions. eg, if `A` is an N×M matrix and `B` is an N×M matrix, then `A * B` will give an N×M matrix.\n",
    "\n",
    "### Matrix functions\n",
    "\n",
    "To create an N×M matrix of zeros, use `np.zeros()`. There is also `np.ones()`, and we are already familiar with `np.random.random()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16477493, 0.73933374, 0.56988856, 0.61678734, 0.23798651],\n",
       "       [0.66301537, 0.49340195, 0.9869649 , 0.42576324, 0.47851367],\n",
       "       [0.55403078, 0.25593087, 0.35251436, 0.3589017 , 0.72616016],\n",
       "       [0.71154164, 0.75759915, 0.77727934, 0.93432638, 0.57690342],\n",
       "       [0.80692695, 0.28318693, 0.74542881, 0.69882094, 0.05988588],\n",
       "       [0.94325423, 0.34524789, 0.91998229, 0.99569443, 0.64020768],\n",
       "       [0.01690838, 0.57429171, 0.57097306, 0.53236099, 0.93365809],\n",
       "       [0.36715088, 0.0425232 , 0.15438888, 0.86086126, 0.23822314],\n",
       "       [0.47338393, 0.5034946 , 0.98155374, 0.20006426, 0.64186293],\n",
       "       [0.67241462, 0.81312395, 0.37143047, 0.22092327, 0.48704332]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, M = 10, 5\n",
    "\n",
    "np.zeros((N, M))\n",
    "np.ones((N, M))\n",
    "np.random.random((N, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have a function that only operates on 1D arrays -- eg, `np.random.dirichlet()`. And let's say you're trying to apply this function to each column of a matrix of ones. You can achieve this with `np.apply_along_axis()`. It takes three arguments: the function to apply, the axis to apply it along, and the matrix to apply it to. As usual, axis=0 is columns, axis=1 is rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02634111, 0.03606111, 0.08038986, 0.06644483, 0.09719608],\n",
       "       [0.05512077, 0.0219442 , 0.01838183, 0.14130039, 0.05208666],\n",
       "       [0.06268665, 0.33896843, 0.39802107, 0.2625763 , 0.02995739],\n",
       "       [0.16458795, 0.16133434, 0.16765693, 0.07146581, 0.23032693],\n",
       "       [0.14233054, 0.06301238, 0.0388472 , 0.03210519, 0.03483534],\n",
       "       [0.08502519, 0.05153743, 0.00934048, 0.02338332, 0.00852954],\n",
       "       [0.31382729, 0.00690901, 0.03169892, 0.05123088, 0.04364808],\n",
       "       [0.03169968, 0.00762102, 0.07350008, 0.19650209, 0.11083772],\n",
       "       [0.03826777, 0.19299961, 0.11342401, 0.0246196 , 0.02327936],\n",
       "       [0.08011304, 0.11961248, 0.06873963, 0.13037159, 0.36930291]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, R = 10, 5\n",
    "W = np.ones((N, R))\n",
    "\n",
    "np.apply_along_axis(np.random.dirichlet, 0, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sum all of a matrix's values with the `.sum()` method. If you want to sum the values in all columns or rows, you can use the `.sum()` method's `axis` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, M = 5, 3\n",
    "X = np.ones((N, M))\n",
    "\n",
    "X.sum()        # --> 30.0\n",
    "X.sum(axis=0)  # --> array([ 5.,  5.,  5.])\n",
    "X.sum(axis=1)  # --> array([ 3.,  3.,  3.,  3.,  3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize the columns of a matrix, we can just divide the matrix by the sum of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 5, 3\n",
    "X = np.random.random((N, M))\n",
    "\n",
    "X /= X.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually a little surprising that this works, given that we're asking to divide a 5×3 matrix by a 3-element vector. NumPy somehow figures out that you actually want it to perform element-wise division between the 1×3 matrix and each row of the 5×3 matrix. But for reasons that are unclear to me, it cannot figure out what you want it to do for a 5×3 matrix and a 5-element vector ... at least not without a little help. To help NumPy figure out what you want, we need to use NumPy broadcasting.\n",
    "\n",
    "Other useful functions to be familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply()\n",
    "np.divide()\n",
    "np.power()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your dimensions as you go!**\n",
    "\n",
    "### NumPy broadcasting for element-wise operations\n",
    "\n",
    "If you have a matrix __A__ with dimensions N×M, and you want to add an M-element array __b__ to all of __A__'s rows, you just need to add another axis to __b__, keeping the two same-sized axes aligned. In this example, `A.shape` is (N, M) and `b.shape` is (M,).\n",
    "\n",
    "To add an axis to __b__, NumPy uses the notation: `B = b[np.newaxis,:]` Since we put the new axis first, `B.shape` will be (1, M). If we had instead done `b[:,np.newaxis]`, then `B.shape` would be (M, 1). But we are trying to make __B__ compatible with __A__, so we made __B__'s second dimension agree with __A__'s second dimension (ie, both are of size M).\n",
    "\n",
    "`np.newaxis` is synonymous with `None`, so you could also do `b[None,:]`.\n",
    "\n",
    "Now that __A__ and __B__ are both 2D matrices, you can just say: `A + B`, and Numpy will replicate the first (and only) row of __B__ N times before performing standard element-wise addition on two N×M matrices. This is best described visually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [ 0 10 20 30] shape: (4,)\n",
      "B: [0 1 2] shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "A = np.array([0, 10, 20, 30])\n",
    "B = np.array([0, 1, 2])\n",
    "\n",
    "print('A:', A, 'shape:', A.shape)\n",
    "print('B:', B, 'shape:', B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new A:\n",
      " [[ 0]\n",
      " [10]\n",
      " [20]\n",
      " [30]] shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# we want to make A.shape to (4, 1)\n",
    "new_A = A[:, np.newaxis]\n",
    "print(\"new A:\\n\", new_A, 'shape:', new_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A+B:\n",
      " [[ 0  1  2]\n",
      " [10 11 12]\n",
      " [20 21 22]\n",
      " [30 31 32]]\n"
     ]
    }
   ],
   "source": [
    "# now we can add A and B\n",
    "print(\"A+B:\\n\", new_A+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Broadcasting is not strictly necessary for this homework. Anything you can do with NumPy broadcasting, you can also do by brute force with a `for` loop; it will just run much slower (~10-100x for this homework)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework hints\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "We want to generate known __W__ and __H__ matrices, and use them to generate a noisy __V__ matrix.\n",
    "\n",
    "For __W__, each row represents a gene, and each column represents a module. Each row should have a non-zero entry in only one column -- unless it is a moonlighting gene in which case it should have a non-zero value in two columns. Each column should sum to one.\n",
    "\n",
    "For __H__, each row represents a module, and each column represents a sample. This can be random, but each column should sum to one. You could generate a ones matrix, and apply `np.random.dirichlet()` along each column.\n",
    "\n",
    "For __V__, find the matrix product of __W__ and __H__, and then add Poisson noise.\n",
    "\n",
    "We'll feed __V__ into our NMF function, and try to recover the true __W__ and __H__.\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "The NMF function should take two arguments: __V__ (the matrix to decompose) and R (the number of modules we think __V__ contains).\n",
    "\n",
    "To decompose __V__, start with randomly generated __W__ and __H__ matrices, and run the NMF update steps until the log likelihood converges.\n",
    "\n",
    "The NMF update equations contain three different subscripts, so the easiest solution is to use three nested for loops for the first __W__ update step, and another three for the __H__ update step.\n",
    "\n",
    "_If that's too easy for you and you're up for a challenge, it is also possible to implement both update steps with nothing but matrix algebra. This runs much faster than the for loop implementation, but it also requires some mathematical intuition and a solid understanding of NumPy broadcasting._\n",
    "\n",
    "__Hint:__ _Both update steps are simply projecting two 2D matrices into 3D via broadcasting, performing element-wise multiplication, and collapsing them back to 2D via summation along the axis shared by the original two matrices._\n",
    "\n",
    "After each update step, we want to calculate the log likelihood that __V__ was generated by our current guess at __W__ and __H__. See Sean's lecture notes for the log likelihood formula.\n",
    "\n",
    "__Note:__ You should not need a for loop to calculate log likelihood. All three matrices in the log likelihood equation are of the same size, so you can use normal arithmetic operators and the log function on your NumPy matrices, and then use the `.sum()` method for the double summation.\n",
    "\n",
    "__Note:__ From step to step, we only care about the gradient of the log likelihood -- ie, how much it has changed -- which means that any constant terms will cancel out. Specifically, there is a log(V!) term that we can ignore, which is nice because __V__! is huge and dealing with it would be a pain.\n",
    "\n",
    "Recover __W__ and __H__ for R=3..6, and compare the resulting log likelihoods. Did you recover the correct R? Can you recover the gene-to-module assignments from __W__? Do you also recover the moonlighting genes?\n",
    "\n",
    "__Hint:__ You'll probably need to row-normalize __W__, and then look at a histogram. Don't forget to `flatten()` your NumPy array before you histogram.\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "If you've made it this far, Problem 3 should be easy. Take the provided data set, and repeat the same steps as Problem 2. Find R, find __W__, identify any moonlighting genes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

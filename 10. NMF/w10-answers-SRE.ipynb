{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answers 10: the adventure of the moonlighting genes\n",
    "**Sean's answers**\n",
    "\n",
    "I've downloaded [Adler's data file, w10-data.tbl](http://mcb112.org/w10/w10-data.tbl) to my current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. write a script that simulates positive control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)   # Make the simulation reproducible (so if we re-run this cell, we don't have to re-run the whole page)\n",
    "\n",
    "R = 4       # Number of hidden components, R >= 2\n",
    "N = 100     # Number of genes. HW used sand mouse names; here we'll just use gene0 - gene99\n",
    "M = 60      # Number of cell types/samples/experiments\n",
    "\n",
    "X = 20      # Assign X genes to components 0..R-2; component R-1 gets the rest.\n",
    "Y = 3       # Sets 0 and 1 share <Y> moonlighting genes\n",
    "\n",
    "# Assign N genes randomly to R components.\n",
    "# Sets 0..R-2 have X=20 genes each; set R-1 gets the rest\n",
    "# Sets 0 and 1 share Y=3 moonlighting genes.\n",
    "#   => so there have to be more than (R-1)*X-Y genes.\n",
    "#\n",
    "assert(N > (R-1)*X-Y)  \n",
    "shuffidx = np.random.permutation(N)    # Randomly shuffle gene indices, to randomly assign genes to sets\n",
    "S   = []\n",
    "S.append(set(shuffidx[0:X]))           # Set 0 gets first X genes\n",
    "S.append(set(shuffidx[X-Y:X*2-Y]))     # Set 1 also has X genes, but overlaps set 0 by Y genes (random, because of the permuted order)\n",
    "i = X*2-Y\n",
    "for a in range(2, R-1):                # Sets 2..R-2 get X genes each\n",
    "    S.append(set(shuffidx[i:i+X]))\n",
    "    i += X\n",
    "S.append(set(shuffidx[i:]))            # Set R-1 gets the rest.\n",
    "\n",
    "# H_ru  for component r, experiment u\n",
    "# Columns u are probability vectors, \\sum_r H_ru = 1\n",
    "#   - np.ones(R) gives us a vector of R values = 1.0 (Dirichlet parameters)\n",
    "#   - sampling from a Dirichlet with all \\alpha_i = 1 samples p_i uniformly\n",
    "#   - size=M argument says do it M times, giving us M row probability vectors: MxR matrix\n",
    "#   - we need an RxM matrix with columns as prob vectors, so .T transposes \n",
    "#\n",
    "H = np.random.dirichlet(np.ones(R), size=M).T\n",
    "\n",
    "\n",
    "# W_ir = P(i | r)    for gene i, component r\n",
    "# Sparse ... for each module r, only some genes are expressed.\n",
    "#\n",
    "W = np.zeros((N,R))\n",
    "for a in range(R):\n",
    "    p = np.random.dirichlet(np.ones(len(S[a])))  # probabilities of the subset that's expressed\n",
    "    for j, i in enumerate(S[a]):                 # assign those probabilities to the subset of expressed genes\n",
    "        W[i,a] = p[j]\n",
    "\n",
    "\n",
    "# C_u : total counts per experiment (900-1100)*N\n",
    "#\n",
    "C = np.zeros(M).astype(int)\n",
    "for u in range(M):\n",
    "    C[u] = (900 + np.random.randint(201)) * N\n",
    "\n",
    "\n",
    "# lambda_iu = C_u \\sum_r W_ir H_ru = \\sum_r P(i | r) P(r)  for each experiment u\n",
    "# (we can't use lambda as a variable name, because it has a meaning in python)\n",
    "#\n",
    "lam = W @ H            # that's matrix multiplication in python3, so I'm told\n",
    "for i in range(N):\n",
    "    for u in range(M):\n",
    "        lam[i,u] *= C[u]  # there's surely a better way to multiply a matrix by a vector in numpy, but this'll do\n",
    "        \n",
    "\n",
    "# V_iu = observed data, Poisson distributed around \\lambda_iu\n",
    "#\n",
    "V  = np.zeros((N,M)).astype(int)\n",
    "for i in range(N):\n",
    "    for u in range(M):\n",
    "        V[i,u] = np.random.poisson(lam[i,u])\n",
    "        \n",
    "        \n",
    "# Let's remember the true solution. We'll be using W,H a lot below.\n",
    "#\n",
    "W_true = np.copy(W)\n",
    "H_true = np.copy(H)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inferred assignments of genes to components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "    1 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "    2 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "    3 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "    4 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "    5 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "    6 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "    7 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "    8 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "    9 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   10 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   11 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   12 0.7821 0.2179 0.0000 0.0000       YES YES   .   .     <= moonlighter\n",
      "   13 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   14 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   15 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   16 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   17 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   18 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   19 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   20 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   21 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   22 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   23 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   24 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   25 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   26 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   27 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   28 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   29 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   30 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   31 0.3412 0.6588 0.0000 0.0000       YES YES   .   .     <= moonlighter\n",
      "   32 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   33 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   34 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   35 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   36 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   37 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   38 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   39 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   40 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   41 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   42 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   43 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   44 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   45 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   46 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   47 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   48 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   49 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   50 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   51 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   52 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   53 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   54 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   55 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   56 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   57 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   58 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   59 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   60 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   61 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   62 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   63 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   64 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   65 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   66 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   67 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   68 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   69 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   70 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   71 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   72 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   73 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   74 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   75 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   76 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   77 0.6988 0.3012 0.0000 0.0000       YES YES   .   .     <= moonlighter\n",
      "   78 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   79 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   80 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   81 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   82 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   83 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   84 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   85 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   86 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   87 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   88 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   89 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   90 1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "   91 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   92 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   93 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   94 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   95 0.0000 0.0000 1.0000 0.0000         .   . YES   . \n",
      "   96 0.0000 1.0000 0.0000 0.0000         . YES   .   . \n",
      "   97 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   98 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "   99 0.0000 0.0000 0.0000 1.0000         .   .   . YES \n",
      "Component  0: 20 genes assigned.\n",
      "Component  1: 20 genes assigned.\n",
      "Component  2: 20 genes assigned.\n",
      "Component  3: 43 genes assigned.\n"
     ]
    }
   ],
   "source": [
    "# It's useful to look at the W's normalized in the a direction:\n",
    "# the posterior probability that component a generates gene i, as\n",
    "# opposed to the other components. We'll reuse this function to \n",
    "# look at optimized solutions; for now we use it to look at the true\n",
    "# solution in our simulated data.\n",
    "# \n",
    "def print_sets(W, H, genenames=[]):\n",
    "    (N,R) = W.shape\n",
    "    (R,M) = H.shape\n",
    "    pp = np.zeros((N,R))\n",
    "    Z  = np.sum(W, axis=1)\n",
    "    for i in range(N):\n",
    "        for a in range(R):\n",
    "            pp[i,a] = W[i,a] / Z[i]\n",
    "            \n",
    "    nassigned = np.zeros(R).astype(int)\n",
    "    for i in range(N):\n",
    "        if len(genenames) > 0:\n",
    "            print(\"{0:15s} \".format(genenames[i]), end='')\n",
    "        else:\n",
    "            print(\"{0:5d} \".format(i), end='')\n",
    "        for a in range(R):\n",
    "            print(\"{0:6.4f} \".format( pp[i,a] ), end='')\n",
    "        print('      ', end='')\n",
    "\n",
    "        nin = 0\n",
    "        for a in range(R):\n",
    "            if pp[i,a] > 0.10:            # Arbitrary threshold for calling a gene \"assigned\" to a component!\n",
    "                print(\"YES \", end='')     # Here, for the true solution, unassigned is a pp=0\n",
    "                nin += 1                  # but when we run NMF, we'll only get pp~0.\n",
    "                nassigned[a] += 1\n",
    "            else:       \n",
    "                print(\"  . \", end='')\n",
    "        if nin > 1:\n",
    "            print(\"    <= moonlighter\", end='')\n",
    "        print('')\n",
    "\n",
    "    for a in range(R):\n",
    "        print(\"Component {0:2d}: {1:2d} genes assigned.\".format(a, nassigned[a]))\n",
    "        \n",
    "        \n",
    "print_sets(W_true, H_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log likelihood of the data\n",
    "\n",
    "We're going to want a function that calculates the log likelihood of an NMF model. Let's go ahead and implement that function now, and see what the log likelihood of the true model is. Our NMF implementation should approach this (it can even do better, by overfitting the noise in the data a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.01408e+07\n"
     ]
    }
   ],
   "source": [
    "# A function for calculating the total likelihood (up to a constant)\n",
    "#\n",
    "def loglikelihood(V, lam):\n",
    "    N,M = V.shape\n",
    "    ll = 0.\n",
    "    for i in range(N):\n",
    "        for u in range(M):\n",
    "            ll += V[i,u] * np.log(lam[i,u]) - lam[i,u]   # \\sum_u \\sum_i log(V_iu ! ) is a constant offset; we neglect it.\n",
    "    return ll\n",
    "\n",
    "print(\"{0:.6g}\".format(loglikelihood(V, lam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF(V, R):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "       V are the data: NxM matrix of mapped read counts\n",
    "       R is the number of hidden components we want\n",
    "       \n",
    "    Returns:\n",
    "       W : the NxR weight matrix\n",
    "       H : the RxM coefficient matrix\n",
    "       ll : log likelihood of the WxH solution\n",
    "       n  : number of iterations it took\n",
    "    \"\"\"\n",
    "    N,M = V.shape\n",
    "    C = np.sum(V, axis=0)\n",
    "    \n",
    "    # Initial guesses for H, W\n",
    "    #\n",
    "    H = np.random.dirichlet(np.ones(R), size=M).T    # gives an RxM matrix, columns are probability vectors\n",
    "    W = np.random.dirichlet(np.ones(N), size=R).T    #  ... and NxR, ditto\n",
    "            \n",
    "    # NMF optimization (following Lee and Seung, 1999, but with explicit counts, and normalized H_au)\n",
    "    #\n",
    "    iterations = 0\n",
    "    while True:   \n",
    "        # Update our expected mean lambda_iu, given current W,H.\n",
    "        #\n",
    "        lam = W @ H\n",
    "        for i in range(N):\n",
    "            for u in range(M):\n",
    "                lam[i,u] *= C[u]  \n",
    "                \n",
    "        # Calculate the log likelihood of the current model W,H, \n",
    "        # and test for convergence.\n",
    "        #\n",
    "        ll = loglikelihood(V, lam)\n",
    "        #print(\"current log likelihood = {0:.3f} \".format( ll))\n",
    "        if iterations >= 1:\n",
    "            diff = (ll - old_ll) / abs(old_ll)\n",
    "            if diff < 1e-8: break\n",
    "        iterations += 1\n",
    "        old_ll      = ll\n",
    "\n",
    "        # Update eqn for W'_ia\n",
    "        #\n",
    "        new_W = np.zeros((N,R))\n",
    "        for i in range(N):\n",
    "            for a in range(R):\n",
    "                term1 = 0.\n",
    "                term2 = 0.\n",
    "                for u in range(M):\n",
    "                    term1 += (V[i,u] / lam[i,u]) * C[u] * H[a,u]\n",
    "                    term2 += C[u] * H[a,u]\n",
    "                new_W[i,a] = W[i,a] * term1 / term2 \n",
    "        \n",
    "        # Renormalization of W_ia\n",
    "        #\n",
    "        Z = np.sum(new_W, axis=0)\n",
    "        for a in range(R):\n",
    "            for i in range(N):\n",
    "                new_W[i,a] = new_W[i,a] / Z[a]\n",
    "\n",
    "        # Update eqn for H'_au\n",
    "        new_H = np.zeros((R,M))\n",
    "        for a in range(R):\n",
    "            for u in range(M):\n",
    "                term1 = 0.\n",
    "                term2 = 0.\n",
    "                for i in range(N):\n",
    "                    term1 += (V[i,u] / lam[i,u]) * W[i,a]\n",
    "                new_H[a,u] = H[a,u] * term1 \n",
    "        \n",
    "        # Now swap the updated estimates in, making them the current model.\n",
    "        W = new_W\n",
    "        H = new_H\n",
    "\n",
    "    return (W,H, ll, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a local optimizer, so let's wrap the NMF optimization in something that runs it many times and takes the solution with the best log likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMF_best(V, R, nruns):\n",
    "    for run in range(nruns):\n",
    "        (W, H, ll, n_iterations) = NMF(V,R)\n",
    "        print (\"run: {0:2d}  iterations: {1:5d}  logL: {2:.6g}\".format(run, n_iterations, ll))\n",
    "        if run == 0 or ll > ll_best:\n",
    "            ll_best = ll\n",
    "            W_best  = np.copy(W)\n",
    "            H_best  = np.copy(H)\n",
    "    return (W_best, H_best, ll_best)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each run takes a while, and doing several runs takes several times as long ... set it running and get a cup of coffee..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:  0  iterations:   399  logL: 4.0141e+07\n",
      "run:  1  iterations:   266  logL: 4.01411e+07\n",
      "run:  2  iterations:   539  logL: 4.0141e+07\n",
      "run:  3  iterations:   431  logL: 4.01411e+07\n",
      "run:  4  iterations:   421  logL: 4.0141e+07\n"
     ]
    }
   ],
   "source": [
    "(W, H, logL) = NMF_best(V,R,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 0.0058 0.9902 0.0000 0.0040         . YES   .   . \n",
      "    1 0.0026 0.0659 0.0009 0.9305         .   .   . YES \n",
      "    2 0.0099 0.0568 0.0013 0.9319         .   .   . YES \n",
      "    3 0.9621 0.0062 0.0091 0.0225       YES   .   .   . \n",
      "    4 0.0007 0.9926 0.0000 0.0067         . YES   .   . \n",
      "    5 0.0009 0.0206 0.9708 0.0077         .   . YES   . \n",
      "    6 0.9631 0.0064 0.0087 0.0218       YES   .   .   . \n",
      "    7 0.9643 0.0039 0.0118 0.0200       YES   .   .   . \n",
      "    8 0.9640 0.0032 0.0120 0.0208       YES   .   .   . \n",
      "    9 0.0001 0.0323 0.9677 0.0000         .   . YES   . \n",
      "   10 0.0000 0.9933 0.0000 0.0066         . YES   .   . \n",
      "   11 0.0001 0.0189 0.9729 0.0081         .   . YES   . \n",
      "   12 0.0018 0.7648 0.2270 0.0064         . YES YES   .     <= moonlighter\n",
      "   13 0.9634 0.0090 0.0097 0.0179       YES   .   .   . \n",
      "   14 0.0072 0.0594 0.0071 0.9263         .   .   . YES \n",
      "   15 0.0116 0.0148 0.9559 0.0177         .   . YES   . \n",
      "   16 0.9621 0.0038 0.0120 0.0221       YES   .   .   . \n",
      "   17 0.9622 0.0064 0.0088 0.0226       YES   .   .   . \n",
      "   18 0.0005 0.9915 0.0000 0.0080         . YES   .   . \n",
      "   19 0.9648 0.0042 0.0116 0.0194       YES   .   .   . \n",
      "   20 0.0059 0.0527 0.0027 0.9387         .   .   . YES \n",
      "   21 0.0005 0.0619 0.0011 0.9365         .   .   . YES \n",
      "   22 0.0003 0.9919 0.0000 0.0078         . YES   .   . \n",
      "   23 0.0051 0.0578 0.0028 0.9343         .   .   . YES \n",
      "   24 0.9647 0.0060 0.0071 0.0222       YES   .   .   . \n",
      "   25 0.9603 0.0053 0.0115 0.0229       YES   .   .   . \n",
      "   26 0.0015 0.0213 0.9716 0.0055         .   . YES   . \n",
      "   27 0.9635 0.0042 0.0114 0.0209       YES   .   .   . \n",
      "   28 0.0021 0.0209 0.9694 0.0076         .   . YES   . \n",
      "   29 0.0101 0.0555 0.0029 0.9315         .   .   . YES \n",
      "   30 0.0021 0.9887 0.0000 0.0092         . YES   .   . \n",
      "   31 0.0013 0.3329 0.6590 0.0067         . YES YES   .     <= moonlighter\n",
      "   32 0.0088 0.0564 0.0015 0.9333         .   .   . YES \n",
      "   33 0.0001 0.9951 0.0000 0.0049         . YES   .   . \n",
      "   34 0.9643 0.0061 0.0090 0.0206       YES   .   .   . \n",
      "   35 0.9620 0.0060 0.0100 0.0220       YES   .   .   . \n",
      "   36 0.0039 0.0617 0.0015 0.9329         .   .   . YES \n",
      "   37 0.0071 0.0542 0.0020 0.9367         .   .   . YES \n",
      "   38 0.9542 0.0068 0.0164 0.0226       YES   .   .   . \n",
      "   39 0.0002 0.9934 0.0000 0.0064         . YES   .   . \n",
      "   40 0.0016 0.0214 0.9733 0.0038         .   . YES   . \n",
      "   41 0.0120 0.0329 0.0013 0.9537         .   .   . YES \n",
      "   42 0.0020 0.0201 0.9700 0.0078         .   . YES   . \n",
      "   43 0.0040 0.0535 0.0042 0.9384         .   .   . YES \n",
      "   44 0.0000 0.9868 0.0000 0.0132         . YES   .   . \n",
      "   45 0.0001 0.9978 0.0000 0.0022         . YES   .   . \n",
      "   46 0.0085 0.0523 0.0031 0.9361         .   .   . YES \n",
      "   47 0.0014 0.0201 0.9698 0.0087         .   . YES   . \n",
      "   48 0.0050 0.0477 0.0052 0.9422         .   .   . YES \n",
      "   49 0.9627 0.0072 0.0094 0.0208       YES   .   .   . \n",
      "   50 0.0128 0.0506 0.0005 0.9361         .   .   . YES \n",
      "   51 0.0095 0.0521 0.0038 0.9346         .   .   . YES \n",
      "   52 0.0058 0.0502 0.0067 0.9373         .   .   . YES \n",
      "   53 0.0000 0.9973 0.0000 0.0026         . YES   .   . \n",
      "   54 0.0112 0.0509 0.0024 0.9354         .   .   . YES \n",
      "   55 0.0027 0.0202 0.9705 0.0066         .   . YES   . \n",
      "   56 0.0049 0.0696 0.0035 0.9221         .   .   . YES \n",
      "   57 0.0108 0.0462 0.0015 0.9415         .   .   . YES \n",
      "   58 0.0078 0.0524 0.0024 0.9374         .   .   . YES \n",
      "   59 0.0097 0.0563 0.0034 0.9306         .   .   . YES \n",
      "   60 0.0125 0.0485 0.0043 0.9347         .   .   . YES \n",
      "   61 0.0137 0.0492 0.0060 0.9311         .   .   . YES \n",
      "   62 0.9409 0.0000 0.0318 0.0272       YES   .   .   . \n",
      "   63 0.0038 0.0561 0.0051 0.9350         .   .   . YES \n",
      "   64 0.0098 0.0505 0.0056 0.9340         .   .   . YES \n",
      "   65 0.9605 0.0038 0.0143 0.0215       YES   .   .   . \n",
      "   66 0.0031 0.0199 0.9632 0.0137         .   . YES   . \n",
      "   67 0.0092 0.0533 0.0007 0.9368         .   .   . YES \n",
      "   68 0.0083 0.0569 0.0038 0.9310         .   .   . YES \n",
      "   69 0.0000 0.0166 0.9707 0.0127         .   . YES   . \n",
      "   70 0.0011 0.9927 0.0000 0.0062         . YES   .   . \n",
      "   71 0.0078 0.0504 0.0017 0.9401         .   .   . YES \n",
      "   72 0.0023 0.0200 0.9720 0.0058         .   . YES   . \n",
      "   73 0.0004 0.9938 0.0000 0.0057         . YES   .   . \n",
      "   74 0.0078 0.0574 0.0074 0.9273         .   .   . YES \n",
      "   75 0.0049 0.0476 0.0035 0.9439         .   .   . YES \n",
      "   76 0.0002 0.9943 0.0000 0.0055         . YES   .   . \n",
      "   77 0.0024 0.6789 0.3125 0.0062         . YES YES   .     <= moonlighter\n",
      "   78 0.9604 0.0059 0.0121 0.0215       YES   .   .   . \n",
      "   79 0.0059 0.0542 0.0082 0.9317         .   .   . YES \n",
      "   80 0.0012 0.9916 0.0000 0.0072         . YES   .   . \n",
      "   81 0.9618 0.0069 0.0120 0.0194       YES   .   .   . \n",
      "   82 0.0156 0.0492 0.0035 0.9317         .   .   . YES \n",
      "   83 0.0001 0.9962 0.0000 0.0037         . YES   .   . \n",
      "   84 0.0154 0.0399 0.0027 0.9419         .   .   . YES \n",
      "   85 0.0021 0.0189 0.9732 0.0058         .   . YES   . \n",
      "   86 0.0053 0.0542 0.0020 0.9385         .   .   . YES \n",
      "   87 0.0090 0.0541 0.0032 0.9336         .   .   . YES \n",
      "   88 0.0000 0.0068 0.9484 0.0448         .   . YES   . \n",
      "   89 0.0124 0.0592 0.0022 0.9262         .   .   . YES \n",
      "   90 0.0000 0.9857 0.0001 0.0142         . YES   .   . \n",
      "   91 0.0074 0.0554 0.0047 0.9326         .   .   . YES \n",
      "   92 0.0077 0.0503 0.0036 0.9385         .   .   . YES \n",
      "   93 0.0011 0.0204 0.9706 0.0079         .   . YES   . \n",
      "   94 0.0042 0.0560 0.0004 0.9394         .   .   . YES \n",
      "   95 0.9626 0.0063 0.0108 0.0202       YES   .   .   . \n",
      "   96 0.0018 0.0191 0.9687 0.0104         .   . YES   . \n",
      "   97 0.0124 0.0557 0.0036 0.9283         .   .   . YES \n",
      "   98 0.0000 0.0621 0.0103 0.9276         .   .   . YES \n",
      "   99 0.0086 0.0673 0.0045 0.9196         .   .   . YES \n",
      "Component  0: 20 genes assigned.\n",
      "Component  1: 20 genes assigned.\n",
      "Component  2: 20 genes assigned.\n",
      "Component  3: 43 genes assigned.\n"
     ]
    }
   ],
   "source": [
    "print_sets(W,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a couple of observations here. \n",
    "\n",
    "One is that NMF sparsifies, but not completely -- when a gene doesn't belong to a component, NMF drives its assignment toward zero, but not completely so. Many implementations of NMF include an additional term in the objective function to drive the factorization toward even more sparse solutions.\n",
    "\n",
    "So to deduce which genes are assigned to which components, the `print_sets` function uses an arbitrary threshold on the posterior probability, of 0.1. It happens to work on this problem because our moonlighting genes are only shared across a couple of components, and we're able to distinguish their assignments easily from the background haze of lower pp's for other genes.\n",
    "\n",
    "And given those semi-arbitrary (i.e. thresholded) assignments, we're usually able to figure out which genes truly belong to which component, even when there's some overlap (i.e., here, three moonlighting genes).\n",
    "\n",
    "We can also note that NMF usually converges to similar answers, on these data, so there isn't much of a local optimum problem. We don't need to do a lot of NMF runs; we can probably get away with fewer. (Which is good, because it takes a while to compute.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can we deduce the number of components?\n",
    "\n",
    "We set the true number of components to R=4. Can we deduce R, by looking at the loglikelihood of our fit, as we vary R? We don't have to do multiple runs of NMF per R choice to get a feel of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:  3  logL: 3.9598e+07\n",
      "R:  4  logL: 4.0141e+07\n",
      "R:  5  logL: 4.01411e+07\n",
      "R:  6  logL: 4.01412e+07\n"
     ]
    }
   ],
   "source": [
    "for r in range(3,7):\n",
    "    (W, H, logL, niter) = NMF(V,r)\n",
    "    print (\"R: {0:2d}  logL: {1:.6g}\".format(r, logL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd say yes, we can. The log likelihood pretty much goes flat at R=4; adding additional components doesn't help it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze the Adler data\n",
    "\n",
    "We have a working NMF implementation, so let's analyze Adler's data set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file into Vx[]\n",
    "# I'm calling the real data Vx[] so I don't overwrite the simulated data from above.\n",
    "#\n",
    "infile = \"w10-data.tbl\"\n",
    "genenames = []\n",
    "Vx        = []\n",
    "with open(infile) as f:\n",
    "    for line in f:\n",
    "        fields = line.split()\n",
    "        genenames.append( fields[0] )\n",
    "        Vx.append( [ float(x) for x in fields[1:] ])\n",
    "Vx     = np.array(Vx)\n",
    "Nx, Mx = Vx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's our best guess for how many gene batteries there are? We've been told the choices are 3..6. Let's look at the log likelihood of NMF solutions for that range of R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R:  3  logL: 4.03928e+07\n",
      "R:  4  logL: 4.07958e+07\n",
      "R:  5  logL: 4.07955e+07\n",
      "R:  6  logL: 4.07961e+07\n"
     ]
    }
   ],
   "source": [
    "for Rx in range(3,7):\n",
    "    (Wx, Hx, logLx, niter) = NMF(Vx,Rx)\n",
    "    print (\"R: {0:2d}  logL: {1:.6g}\".format(Rx, logLx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 4 components, because adding more components after that barely changes the log likelihood. Recompute that (and let's take the best of three runs), so we can look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:  0  iterations:   351  logL: 4.0796e+07\n",
      "run:  1  iterations:   240  logL: 4.07959e+07\n",
      "run:  2  iterations:   384  logL: 4.07959e+07\n",
      "anise           0.0000 0.9912 0.0001 0.0087         . YES   .   . \n",
      "apricot         0.0010 0.0000 0.9917 0.0072         .   . YES   . \n",
      "artichoke       0.9828 0.0109 0.0056 0.0008       YES   .   .   . \n",
      "arugula         0.9838 0.0162 0.0000 0.0000       YES   .   .   . \n",
      "asparagus       0.0030 0.0000 0.9941 0.0029         .   . YES   . \n",
      "avocado         0.9821 0.0116 0.0049 0.0014       YES   .   .   . \n",
      "banana          0.9871 0.0104 0.0025 0.0000       YES   .   .   . \n",
      "basil           0.0234 0.0105 0.0000 0.9661         .   .   . YES \n",
      "beet            0.9839 0.0129 0.0032 0.0000       YES   .   .   . \n",
      "blackberry      0.9806 0.0131 0.0042 0.0020       YES   .   .   . \n",
      "blueberry       0.0026 0.0012 0.9913 0.0049         .   . YES   . \n",
      "broccoli        0.9857 0.0118 0.0017 0.0008       YES   .   .   . \n",
      "butternut       0.0251 0.0135 0.0000 0.9614         .   .   . YES \n",
      "cabbage         0.0217 0.0078 0.0000 0.9705         .   .   . YES \n",
      "cantaloupe      0.0014 0.9839 0.0000 0.0147         . YES   .   . \n",
      "caraway         0.9844 0.0115 0.0040 0.0000       YES   .   .   . \n",
      "carrot          0.0039 0.0000 0.9921 0.0040         .   . YES   . \n",
      "cauliflower     0.0054 0.0001 0.9919 0.0026         .   . YES   . \n",
      "cayenne         0.0014 0.0000 0.9935 0.0052         .   . YES   . \n",
      "celery          0.0227 0.0081 0.0000 0.9692         .   .   . YES \n",
      "chard           0.0219 0.0131 0.0000 0.9650         .   .   . YES \n",
      "cherry          0.0214 0.0034 0.0000 0.9752         .   .   . YES \n",
      "chestnut        0.0099 0.9901 0.0000 0.0000         . YES   .   . \n",
      "chickpea        0.0017 0.0000 0.9938 0.0045         .   . YES   . \n",
      "cilantro        0.9690 0.0182 0.0035 0.0093       YES   .   .   . \n",
      "clementine      0.0040 0.0015 0.9918 0.0027         .   . YES   . \n",
      "coconut         0.0027 0.0000 0.9930 0.0043         .   . YES   . \n",
      "coriander       0.0017 0.0001 0.9946 0.0035         .   . YES   . \n",
      "cranberry       0.0000 0.9971 0.0000 0.0029         . YES   .   . \n",
      "cucumber        0.9811 0.0089 0.0054 0.0046       YES   .   .   . \n",
      "currant         0.9640 0.0209 0.0151 0.0000       YES   .   .   . \n",
      "eggplant        0.9797 0.0116 0.0058 0.0029       YES   .   .   . \n",
      "elderberry      0.9828 0.0080 0.0092 0.0000       YES   .   .   . \n",
      "endive          0.0006 0.0001 0.9942 0.0051         .   . YES   . \n",
      "fennel          0.0234 0.0145 0.0001 0.9619         .   .   . YES \n",
      "fig             0.0000 0.9944 0.0000 0.0056         . YES   .   . \n",
      "garlic          0.0232 0.0124 0.0000 0.9643         .   .   . YES \n",
      "ginger          0.9815 0.0114 0.0058 0.0012       YES   .   .   . \n",
      "gooseberry      0.9769 0.0121 0.0109 0.0000       YES   .   .   . \n",
      "grape           0.0042 0.1150 0.8734 0.0074         . YES YES   .     <= moonlighter\n",
      "grapefruit      0.9832 0.0114 0.0035 0.0018       YES   .   .   . \n",
      "guava           0.9857 0.0122 0.0020 0.0001       YES   .   .   . \n",
      "honeydew        0.9778 0.0061 0.0158 0.0003       YES   .   .   . \n",
      "horseradish     0.9863 0.0103 0.0031 0.0003       YES   .   .   . \n",
      "huckleberry     0.0224 0.0126 0.0000 0.9650         .   .   . YES \n",
      "juniper         0.9836 0.0107 0.0051 0.0007       YES   .   .   . \n",
      "kiwi            0.9831 0.0124 0.0042 0.0003       YES   .   .   . \n",
      "kohlrabi        0.0008 0.4157 0.5762 0.0073         . YES YES   .     <= moonlighter\n",
      "lavender        0.0012 0.0000 0.9937 0.0051         .   . YES   . \n",
      "leek            0.0002 0.9911 0.0001 0.0086         . YES   .   . \n",
      "lentil          0.0003 0.9911 0.0001 0.0085         . YES   .   . \n",
      "lettuce         0.9865 0.0135 0.0000 0.0000       YES   .   .   . \n",
      "lime            0.9862 0.0112 0.0024 0.0003       YES   .   .   . \n",
      "maize           0.9827 0.0140 0.0030 0.0004       YES   .   .   . \n",
      "mango           0.0032 0.0000 0.9937 0.0031         .   . YES   . \n",
      "melon           0.0012 0.9919 0.0011 0.0058         . YES   .   . \n",
      "mulberry        0.0225 0.0131 0.0001 0.9643         .   .   . YES \n",
      "mushroom        0.0223 0.0131 0.0001 0.9645         .   .   . YES \n",
      "mustard         0.9792 0.0130 0.0077 0.0001       YES   .   .   . \n",
      "nectarine       0.0215 0.0105 0.0000 0.9679         .   .   . YES \n",
      "okra            0.9855 0.0134 0.0010 0.0001       YES   .   .   . \n",
      "olive           0.0186 0.0093 0.0001 0.9720         .   .   . YES \n",
      "onion           0.0000 0.0013 0.9987 0.0000         .   . YES   . \n",
      "orange          0.0002 0.9896 0.0004 0.0099         . YES   .   . \n",
      "oregano         1.0000 0.0000 0.0000 0.0000       YES   .   .   . \n",
      "papaya          0.0001 0.9923 0.0007 0.0070         . YES   .   . \n",
      "parsley         0.0013 0.9927 0.0005 0.0055         . YES   .   . \n",
      "parsnip         0.9821 0.0149 0.0031 0.0000       YES   .   .   . \n",
      "pea             0.0043 0.0000 0.9866 0.0091         .   . YES   . \n",
      "peach           0.9852 0.0121 0.0027 0.0000       YES   .   .   . \n",
      "pear            0.0249 0.0136 0.0043 0.9572         .   .   . YES \n",
      "pepper          0.0025 0.2148 0.7771 0.0055         . YES YES   .     <= moonlighter\n",
      "persimmon       0.0050 0.0000 0.9917 0.0032         .   . YES   . \n",
      "pineapple       0.9821 0.0137 0.0040 0.0002       YES   .   .   . \n",
      "plantain        0.0002 0.9915 0.0001 0.0082         . YES   .   . \n",
      "plum            0.0202 0.0088 0.0036 0.9674         .   .   . YES \n",
      "pomegranate     0.0002 0.9931 0.0006 0.0060         . YES   .   . \n",
      "potato          0.9808 0.0125 0.0063 0.0004       YES   .   .   . \n",
      "pumpkin         0.0173 0.0157 0.0000 0.9670         .   .   . YES \n",
      "quince          0.9818 0.0117 0.0065 0.0001       YES   .   .   . \n",
      "radish          0.0325 0.0102 0.0072 0.9501         .   .   . YES \n",
      "raisin          0.9808 0.0123 0.0061 0.0008       YES   .   .   . \n",
      "raspberry       0.0225 0.0116 0.0001 0.9659         .   .   . YES \n",
      "rhubarb         0.0243 0.0092 0.0000 0.9665         .   .   . YES \n",
      "rosemary        0.9848 0.0117 0.0034 0.0000       YES   .   .   . \n",
      "rutabaga        0.9837 0.0122 0.0006 0.0035       YES   .   .   . \n",
      "sage            0.9840 0.0138 0.0014 0.0008       YES   .   .   . \n",
      "scallion        0.0002 0.9917 0.0009 0.0072         . YES   .   . \n",
      "spinach         0.9860 0.0113 0.0022 0.0005       YES   .   .   . \n",
      "strawberry      0.9851 0.0112 0.0035 0.0001       YES   .   .   . \n",
      "tamarind        0.0025 0.0001 0.9924 0.0050         .   . YES   . \n",
      "tangerine       0.0198 0.0062 0.0054 0.9686         .   .   . YES \n",
      "thyme           0.9855 0.0126 0.0019 0.0000       YES   .   .   . \n",
      "tomato          0.9824 0.0107 0.0070 0.0000       YES   .   .   . \n",
      "turnip          0.9400 0.0067 0.0000 0.0533       YES   .   .   . \n",
      "wasabi          0.9837 0.0094 0.0011 0.0059       YES   .   .   . \n",
      "watercress      0.9845 0.0119 0.0033 0.0004       YES   .   .   . \n",
      "watermelon      0.0044 0.9939 0.0000 0.0017         . YES   .   . \n",
      "yam             0.0002 0.9914 0.0001 0.0084         . YES   .   . \n",
      "zucchini        0.0026 0.9875 0.0025 0.0074         . YES   .   . \n",
      "Component  0: 43 genes assigned.\n",
      "Component  1: 20 genes assigned.\n",
      "Component  2: 20 genes assigned.\n",
      "Component  3: 20 genes assigned.\n"
     ]
    }
   ],
   "source": [
    "Rx = 4\n",
    "(Wx, Hx, logLx) = NMF_best(Vx, Rx, 3)\n",
    "print_sets(Wx, Hx, genenames=genenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three moonlighting genes: _grape, kohlrabi, and pepper._\n",
    "\n",
    "Three of the components have 20 genes assigned; the other has 43. \n",
    "\n",
    "(Which happens to be the same as my simulation, because I used the same simulation code that generated Adler's data! That's a little dangerous - better, I should be running simulations with different parameters, to be sure my NMF inferences are robust.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

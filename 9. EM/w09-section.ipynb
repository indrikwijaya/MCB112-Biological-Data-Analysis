{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: EM\n",
    "Notes by Mary Richardson (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization\n",
    "\n",
    "We've discussed several applications of EM in this course so far:\n",
    "1. gene expression clustering (soft k-means or mixture models)\n",
    "2. RNA-seq read mapping\n",
    "3. motif finding\n",
    "\n",
    "EM is a very powerful tool for optimizing parameters when you are missing critical and dependent information. For example:\n",
    "1. for gene expression clustering, we don't know the true centroids or the datapoints that should be assigned to each cluster\n",
    "2. for read mapping, we don't know the expected read counts for each transcript or the transcript abundances\n",
    "3. for motif finding, we don't know the expected letter counts for each motif position or the motif letter frequencies\n",
    "\n",
    "EM let's us update these unknown quantities simultaneously. In the **expectation** step, we compute the probability distribution using current parameters and often assign \"counts\" of some sort:\n",
    "1. calculate probabilities of each gene belonging to each cluster\n",
    "2. calculate expected read counts for each transcript\n",
    "3. calculate expected letter counts for each motif\n",
    "\n",
    "In the **maximization** step, we update the parameters based on the results of the expectation step:\n",
    "1. update centroid locations\n",
    "2. update nucleotide abundances\n",
    "3. update motif letter frequencies\n",
    "\n",
    "This continues in a cycle until convergence, which by now is probably a familiar idea to you.\n",
    "\n",
    "If you're interested in other applications of EM in computational biology (there are lots!), [this](https://www.nature.com/articles/nbt1406) article provides a helpful though not comprehensive review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables\n",
    "\n",
    "Let's start by summarizing what we know about the model we're going to build.\n",
    "\n",
    "We **observe** the the read sequence:\n",
    "- $R$ is the observed read sequence\n",
    "\n",
    "We have **hidden** (or **latent**) variables that we don't directly observe, but that the model uses to generate the observed data:\n",
    "\n",
    "- $G$ is the transcript isoform (ranges from $i = 1..M$)\n",
    "- $S$ is the start position of the read (ranges from $s = 1..L_i$)\n",
    "\n",
    "We want to **infer** the expression level of each transcript:\n",
    "- $\\theta$ represents the parameters, which are the _nucleotide_ abundances $\\nu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic graphical models\n",
    "\n",
    "#### Now let's look at the probabilistic graphical model to understand the relationship between our random variables.\n",
    "\n",
    "**Probabilistic graphical models** (aka Bayesian networks) are directed acyclic graphs (DAGs). In this problem set we know the underlying causal relationships between the variables, so we use a directed graph to represent our model. Probabilistic graphical models allow us to visualize the dependencies of random variables and see which variables are independent of the other variables. This helps us see the assumptions of our model (and as you saw in lecture, it helps us simplify the joint probability calculation!) The shaded variable is our **observed** data.\n",
    "\n",
    "<img src=\"model.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, a word of caution...\n",
    "\n",
    "Expression levels are given in transcript abundance ($\\tau$) in input and output, but we need them to be in nucleotide abundance ($\\nu$) for our generative model. Remember that you can convert between them using the following equation:\n",
    "\n",
    "$$\\nu_i = \\frac{\\tau L_i}{\\sum_i{\\tau L_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use the model drawn above to simulate positive control data that we can later use to test our algorithm.\n",
    "\n",
    "- **Step 1**: Select a transcript isoform $G=i$ with probability $\\nu_i$\n",
    "\n",
    "\n",
    "- **Step 2**: Select a start position $S=s$ with probability $\\frac{1}{L_i-\\ell}$ (assuming a uniform distribution)\n",
    "\n",
    "\n",
    "- **Step 3**: Generate a read sequence $R_n$, given $G=i$ and $S=s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reads(N, theta):\n",
    "    '''\n",
    "    Given N (the number of reads to sample),\n",
    "    and theta, which includes nu (nucleotide abundance) and L (transcript lengths),\n",
    "    Simulate reads\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Likelihood of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start by writing the likelihood of a single read. \n",
    "\n",
    "We want to find the probability that the model generates the observed reads $R$, given the parameters of the model $\\theta$:\n",
    "\n",
    "$$P(R|\\theta)$$\n",
    "\n",
    "But what we actually have is:\n",
    "\n",
    "$$P(R,S,G|\\theta)$$\n",
    "\n",
    "So we need to marginalize over the **hidden** variables in our problem ($S$ and $G$):\n",
    "\n",
    "$$P(R|\\theta) = \\sum_S \\sum_G P(R,S,G|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's rewrite this likelihood equation in terms of probabilities we can actually calulate.\n",
    "\n",
    "We can expand our likelihood into a product of conditional probabilities:\n",
    "\n",
    "$$P(R|\\theta) = \\sum_S\\sum_G{P(R|S,G,\\theta)P(S|G,\\theta)P(G|\\theta)}$$\n",
    "\n",
    "\n",
    "Using the graphical model above, we can determine the following:\n",
    "\n",
    "- The probability of selecting isoform $G$ is conditional on $\\theta$:  \n",
    "$$P(G|\\theta)$$\n",
    "\n",
    "\n",
    "- $S$ and $\\theta$ are conditionally independent given $G$:  \n",
    "$$P(S|G,\\theta) = P(S|G)$$\n",
    "\n",
    "\n",
    "- $R$ is conditionally independent of $\\theta$ given $S$ and $G$:  \n",
    "$$P(R|S,G,\\theta) = P(R|S,G)$$\n",
    "\n",
    "Using these independence assumptions, we can simplify our probability:\n",
    "\n",
    "$$P(R|\\theta) = \\sum_S\\sum_G{P(R|S,G)P(S|G)P(G|\\theta)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's write these conditional probabilities in terms of variables we know.\n",
    "\n",
    "$P(R|S,G)$ is the probability of a read given the segment and gene. \n",
    "- We either observe the read sequence at the given position in the transcript \n",
    "$$P(R|S,G) = 1$$ \n",
    "- Or we don't\n",
    "$$P(R|S,G) = 0$$\n",
    "\n",
    "We can write this more generally:\n",
    "$$P(R|S,G) =  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & \\text{if } R[1..\\ell] = G_i[s..s+\\ell] \\\\\n",
    "      0 & \\text{otherwise} \\\\\n",
    "\\end{array} \n",
    "\\right.$$\n",
    "\n",
    "$P(S|G)$ is the probability that a segment $S$ is in the given transcript $G$. This should be inversely propositional to the length of that transcript:\n",
    "\n",
    "$$P(S|G) = \\frac{1}{L_i-\\ell}$$\n",
    "\n",
    "$P(G|\\theta)$ is the probability of a transcript given the parameters, which is equal to the nucleotide abundance of that transcript:\n",
    "\n",
    "$$P(G|\\theta) = \\nu_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let's write the likelihood of a single read.\n",
    "\n",
    "Now our likelihood calculation is:\n",
    "\n",
    "$$P(R|\\theta) = \\sum_S\\sum_G P(R,S,G|\\theta) = \\sum_S\\sum_G P(R|S,G)P(S|G)P(G|\\theta) = \\sum_S\\sum_G \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{\\nu_i}{L_i-\\ell} & \\text{if } R[1..\\ell] = G_i[s..s+\\ell] \\\\\n",
    "      0 & \\text{otherwise} \\\\\n",
    "\\end{array} \n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now find the total likelihood of all reads.\n",
    "\n",
    "We assume each read is independent. Therefore, our total likelihood is the product of the likelihood of each individual read:\n",
    "$$Likelihood = \\prod_N{P(R_n|\\theta})$$\n",
    "\n",
    "So in log space:\n",
    "$$LogLikelihood = \\sum_N{logP(R_n|\\theta})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(R, theta):\n",
    "    '''\n",
    "    Given R (reads) and \n",
    "    theta (nucleotide abundances and transcript lengths),\n",
    "    Calculate the total log likelihood of the model\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Since we have a \"classic chicken and the egg problem,\" we can use EM to optimize our parameters.\n",
    "\n",
    "We want to find which transcript each of the reads came from. If we knew the counts of reads mapping to each transcript, we could estimate the parameters $\\theta$. If we knew $\\theta$, we could estimate the counts. But we don't know either one. So we use EM! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's estimate the expected counts.\n",
    "\n",
    "For the **expectation** step, we infer the expected counts $c_i$ mapping to each transcript, given the current parameters $\\theta$. This requires calculating the probability that a read mapped to each transcript:\n",
    "$$P(G|R,\\theta) = \\frac{\\sum_S P(R,G,S|\\theta)}{\\sum_G \\sum_S P(R,G,S|\\theta)}$$\n",
    "\n",
    "But we know from our likelihood calculation that $P(R,G,S|\\theta)$ is, so:\n",
    "\n",
    "$$P(G|R,\\theta) = \\frac{\\sum_S \\frac{\\nu_i}{L_i-\\ell} \\text{ if } R[1..\\ell] = G_i[s..s+\\ell]} {\\sum_S\\sum_G \\frac{\\nu_i}{L_i-\\ell} \\text{ if } R[1..\\ell] = G_i[s..s+\\ell]}$$\n",
    "\n",
    "Now to get the counts assigned to each transcript, we sum over the probabilities for all reads:\n",
    "\n",
    "$$c_i = \\sum_n P(G_n=i|R_n,\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(R, theta):\n",
    "    '''\n",
    "    Given R (reads) and \n",
    "    theta (nucleotide abundances and transcript lengths),\n",
    "    Calculate expected counts\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's update the nucleotide abundances.\n",
    "\n",
    "For the **maximization** step, we calculate the updated $\\theta$, given the $c_i$ from the previous expectation step. To do this, we simply normalize the counts in order to get nucleotide abundances:\n",
    "$$\\nu_i = c_i/\\sum_j c_j = c_i/N$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization(C):\n",
    "    '''\n",
    "    Given C (read counts) and \n",
    "    Caluclate the updated nu (nucleotide abundances)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we're ready to run EM! \n",
    "\n",
    "- **Step 1**: initialize $\\theta$ to anything (can be random)\n",
    "\n",
    "\n",
    "- **Step 2**: (expectation) calculate $c_i = \\sum_n P(G_n=i|R_n,\\theta)$\n",
    "\n",
    "\n",
    "- **Step 3**: (maximization) calculate updated $\\theta_i = c_i/N$\n",
    "\n",
    "\n",
    "- **Step 4**: repeat EM until converged\n",
    "\n",
    "\n",
    "EM finds a local optimum, *but* Li et al. (2010) show that there is only a single optimum for $\\theta$ so we will be finding the global optimum in this case. This means we can initialize our parameters however we want as long as the nucleotide abundances sum to one. (**Hint:** check out the section in the lecture notes on the Dirichlet distribution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_maximization(R, theta):\n",
    "    '''\n",
    "    Given R (reads) and \n",
    "    theta (nucleotide abundances and transcript lengths),\n",
    "    Run EM until convergence\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One final tip...\n",
    "For this problem you should just operate with nucleotide abundances during the EM and then convert to transcript abundances after convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nucleotide abundance to transcript abundance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

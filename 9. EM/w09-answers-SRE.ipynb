{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answers 09: the adventure of the ten Arcs\n",
    "**Sean's answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a simulator as a positive control\n",
    "\n",
    "> Write a script that simulates N=1000000 observed read counts, following the model specified above, \n",
    "> for any Arc locus structure (i.e. lengths $L_t$) and any transcript abundances $\\tau$. (You can assume that \n",
    "> there are 10 segments and isoforms, though my version of this script will allow that to vary too.)\n",
    "> Use your script to generate a test data set for known model parameters $\\tau,L$\n",
    "> that youâ€™ve chosen.\n",
    "\n",
    "**The point I want you to get from this: when you are thinking of data analysis in terms of a generative probability model of the data, you automatically have a way to generate positive control data: sample synthetic data from your model.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write my generator so it takes two controlling arguments: a random number seed (so I can generate the same data reproducibly from run to run, for a given seed), and a flag for whether to use the pset's Arc transcript lengths, or to generate new ones. I have them at the top of my notebook, so I can change them and rerun the page.\n",
    "\n",
    "I also set up some other parameters of the problem here: the number of Arc segments, and the number of reads to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed  = 2020\n",
    "do_pset_L = False\n",
    "\n",
    "S         = 10        # Number of segments in the Arc locus \n",
    "T         = 10        # Number of different transcripts T (is the same, one starting on each segment)   \n",
    "N         = 1000000   # total number of observed reads to generate: \\sum_k r_s\n",
    "\n",
    "Slabel  = list(string.ascii_uppercase)[:S]               # ['A'..'J']        : the upper case labels for Arc locus segments \n",
    "Tlabel  = [ \"Arc{}\".format(d) for d in range(1,T+1) ]    # ['Arc1'..'Arc10'] : the labels for Arc transcript isoforms\n",
    "Rlabel  = list(string.ascii_lowercase)[:T]               # ['a'..'j']        : lower case labels for reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"structure\" of an Arc locus is defined by T and by an array of lengths of each Arc transcript. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Arc_structure(do_pset_L, T):  \n",
    "    if do_pset_L:\n",
    "        assert(T == 10)\n",
    "        L    = np.array([ 4, 2, 3, 4, 4, 3, 2, 2, 3, 3 ])    # lengths matching the pset figure\n",
    "    else:\n",
    "        L    = [ np.random.randint(2,5) for _ in range(T) ]  # or sampling: each isoform t is 2..4 segments long; P(S|T) = 1/L_t\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make our life easier at some steps by defining some auxiliary data, dependent on the structure of the Arc problem. `seg_is_in[t][s]` is a Kronecker delta function evaluating to 1 if segment s is in transcript t, else 0. This gets used in the EM algorithm and the log likelihood calculation. `segusage[s]` is the number of Arc transcripts that contain segment s. This gets used in Lestrade's naive solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_auxiliary_arrays(L, T, S):\n",
    "    seg_is_in = np.zeros((T,S)).astype(int)\n",
    "    segusage  = np.zeros(S).astype(int)\n",
    " \n",
    "    for t in range(T):\n",
    "        for s in range(t,t+L[t]):\n",
    "            seg_is_in[t][s%S] = 1\n",
    "            segusage[s%S]    += 1\n",
    "            \n",
    "    return seg_is_in, segusage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pset dictates that expression levels are in transcript abundance ($\\tau$) in input and output, but we need them to be in nucleotide abundance ($\\nu$) for the generative model. Here's a couple of functions to convert back and forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nu(tau, L):\n",
    "    nu = np.multiply(tau, L)              \n",
    "    nu = np.divide(nu, np.sum(nu))\n",
    "    return nu\n",
    "\n",
    "def to_tau(nu, L):\n",
    "    tau = np.divide(nu, L)              \n",
    "    tau = np.divide(tau, np.sum(tau))\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test dataset consists of sampling $\\tau$, then using those known $\\tau$ to sample counts $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample known tau, then generate the observed \"read\" counts, r\n",
    "# The generative model says there's two steps:\n",
    "#   P(T)     = nu    : probability of selecting isoform t (~ expression level of t, in *nucleotides*)\n",
    "#   P(S | T) = 1/L_t : probability of selecting segment s in transcript t\n",
    "# So we just go through these two steps.\n",
    "#\n",
    "def generate_test_data(rng_seed, T, L, N):\n",
    "    np.random.seed(rng_seed)\n",
    "    \n",
    "    tau = np.random.dirichlet(np.ones(T))      # true transcript abundances \\tau\n",
    "    nu  = to_nu(tau, L)\n",
    "\n",
    "    r = np.zeros(S).astype(int)\n",
    "    for n in range(N):                         # for each independent read that we sample:\n",
    "        t = np.random.choice(T, p=nu)          # choice of isoform t = 0..T-1, using \\nu expression levels\n",
    "        s = (t + np.random.choice(L[t])) % S   # choice of segment s = 0..L[t]-1 in isoform, using 1/L[t] uniform distribution\n",
    "        r[s] += 1                              # count one mapped read, 0..S-1\n",
    "        \n",
    "    return(tau, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that's everything we need (and more) for part 1. Now use these functions to generate a test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L                   = generate_Arc_structure(do_pset_L, T)\n",
    "true_tau, r         = generate_test_data(rng_seed, T, L, N)\n",
    "true_nu             = to_nu(true_tau, L)\n",
    "seg_is_in, segusage = make_auxiliary_arrays(L, T, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 3, 3, 2, 2, 2, 3, 4, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39212232, 0.18895911, 0.06517565, 0.02900489, 0.03756561,\n",
       "       0.02236103, 0.02958956, 0.03845195, 0.1811869 , 0.01558298])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([204842, 264696,  88979,  98645, 111545,  30982,  18147,  24019,\n",
       "        76691,  81454])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the log likelihood\n",
    "\n",
    "We're also asked to use Lestrade's script to estimate parameters by their method. On our test data, we calculate the log likelihood of the true parameters versus Lestrade's parameters. We're going to see that Lestrade's solution is crappy, as you probably guessed already.\n",
    "\n",
    "We could run Lestrade's script externally, but let's pull in its code here as a function.\n",
    "\n",
    "**A point I'm emphasizing with this: if we're trying to estimate unknown parameters of a model, one standard thing to do is to find parameters that maximize the likelihood of our model: \"ML\" parameter estimates.** It's very typical to have a function that calculates the total probability $\\log P(D \\mid \\theta)$ of your observed data $D$ and some current model parameters $\\theta$. There are various ways to optimize $\\theta$. Sometimes we'll directly optimize the parameters, either analytically or numerically. Problems where we'd use expectation maximization to find ML parameters arise when we have additional latent random variables whose values we're not interested in, but which we need to know to estimate our parameters. Here, we're missing the true assignment of read counts r to isoforms T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the total log likelihood of the\n",
    "# observed data, given nucleotide abundances nu.\n",
    "#\n",
    "#   P(S) = \\sum_T P(S,T) = \\sum_T P(S | T) P(T) =  \\sum_t (\\nu_t / L_t)  \\delta(S is in T=t)\n",
    "#\n",
    "def loglikelihood(r, nu, L, T, S, seg_is_in):\n",
    "    \"\"\" \n",
    "    Function: loglikelihood()  \n",
    "    Calculates the likelihood of the observed read counts r_1..r_S, \n",
    "    given current expression level parameters nu_1..nu_T. \n",
    "    Leaves off the multinomial coefficient, because it's a constant\n",
    "    that depends only on the given data, not on the abundance parameters.\n",
    "    \"\"\"  \n",
    "    logL     = 0\n",
    "    for s in range(S):                         # Likelihood of getting r_s observed counts for each read sequence a..j \n",
    "        p = 0.\n",
    "        for t in range(T):                     # ... summed over possible transcripts Ti\n",
    "            if seg_is_in[t][s]:\n",
    "                p += nu[t] / L[t]              # P(S) = \\sum_T P(T,S) \n",
    "        logL += r[s] * np.log(p)\n",
    "    return logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lestrade_estimate(r, L, T, S, segusage):\n",
    "    \"\"\"\n",
    "    Function: lestrade_estimate()\n",
    "    Lestrade et al. simply assign each read to each isoform it could have come from,\n",
    "    equally distributing the one read count, ignoring the read accuracy.\n",
    "    \"\"\"\n",
    "    c  = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        for s in range(t,t+L[t]):\n",
    "            c[t] += (1.0 / float(segusage[s%S])) * float(r[s%S])  # For each read s, assign 1/usage count to each transcript\n",
    "                                                                  # that contains segment j.\n",
    "    Z      = np.sum(c)\n",
    "    est_nu = np.divide(c, Z)\n",
    "    return est_nu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run Lestrade's method on our generated test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lestrade's estimates:\n",
      "isoform       est_tau   true_tau\n",
      "Arc1            0.221      0.392\n",
      "Arc2            0.137      0.189\n",
      "Arc3            0.099      0.065\n",
      "Arc4            0.067      0.029\n",
      "Arc5            0.054      0.038\n",
      "Arc6            0.027      0.022\n",
      "Arc7            0.030      0.030\n",
      "Arc8            0.073      0.038\n",
      "Arc9            0.157      0.181\n",
      "Arc10           0.135      0.016\n",
      "\n",
      "logL, est  = -2073371.9\n",
      "logL, true = -2036190.9\n"
     ]
    }
   ],
   "source": [
    "nu_lestrade  = lestrade_estimate(r, L, T, S, segusage)\n",
    "tau_lestrade = to_tau(nu_lestrade, L)                     \n",
    "\n",
    "# Print a table of the resulting estimates for tau\n",
    "print(\"Lestrade's estimates:\")\n",
    "print(\"{0:10s}  {1:>9s}  {2:>9s}\".format(\"isoform\", \"est_tau\", \"true_tau\"))\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:9.3f}  {2:9.3f}\".format(Tlabel[i], tau_lestrade[i], true_tau[i]))\n",
    "print('')\n",
    "\n",
    "print(\"logL, est  = {0:.1f}\".format(loglikelihood(r, nu_lestrade, L, T, S, seg_is_in)))\n",
    "print(\"logL, true = {0:.1f}\".format(loglikelihood(r, true_nu,     L, T, S, seg_is_in)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lestrade solution has a worse (more negative) log likelihood than the true parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. estimate isoform abundances by EM\n",
    "\n",
    "> Write a script that estimates unknown isoform abundances $\\tau_i$ for each isoform Arc1..Arc10, \n",
    "> given read counts $r_k$ and the structure of the Arc locus including the lengths $L_i$,\n",
    "> using expectation maximization.\n",
    "> Apply your script to the data in the Lestrade et al. supplementary data file. Show your estimated $\\tau_i$.\n",
    "\n",
    "We could parse the Lestrade supplementary data file - grabbing it through `urllib` even - and Lestrade's\n",
    "own script provided the parsing code we could use, but it's easy enough to just drop its data in here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis by expectation maximization.\n",
    "#\n",
    "# If you told me nu[]; then I could tell you the expected number of\n",
    "# reads mapping to isoform i, c[t], because you can calculate P(T|S).\n",
    "# (Expectation step.)\n",
    "#\n",
    "# If you told me c[]; then I could tell you the maximum likelihood\n",
    "# estimate of nu[], which is simply the normalized counts. \n",
    "# (Maximization step.)\n",
    "#\n",
    "# But I tell you neither; I only tell you r[], the observed reads.\n",
    "#\n",
    "def expectation_maximization(r, L, T, S, seg_is_in):\n",
    "    est_nu      = np.random.dirichlet(np.ones(T))   # random starting guess.\n",
    "    converged   = False\n",
    "    iterations  = 0\n",
    "    while not converged:\n",
    "        iterations += 1\n",
    "    \n",
    "        # Expectation step. \n",
    "        # We need P(T | S): expected prob that a read mapped to S came from transcript T.\n",
    "        #   P(T|S) =  P(T,S) / \\sum_T P(T,S) = nu_i * 1/L_i if segment S is in T, normalize over T. \n",
    "        c = np.zeros(T)\n",
    "        for s in range(S):      # For a given choice of read S, where we've observed r_s counts\n",
    "            Pr = np.zeros(T)    #   ... we're going to calculate P(T|S)\n",
    "            for t in range(T):  #     ... by first calculating the numerator P(T,S) = \\nu_i/L_i for the S in T\n",
    "                if seg_is_in[t][s]:\n",
    "                    Pr[t] +=  est_nu[t] / float(L[t])  \n",
    "            Z  = np.sum(Pr)            # ... now Z = P(S) \n",
    "            Pr = np.divide(Pr, Z)      # ... normalizing gives us P(T | S) = P(S,T) / P(S), for this read s\n",
    "\n",
    "            # Now <Pr> is P(T=t | S=s), for read s; we dole out the observed counts\n",
    "            # r[s] to the possible transcripts they came\n",
    "            # from, proportional to the probability they came from that one.\n",
    "            #\n",
    "            for t in range(T):\n",
    "                c[t] += Pr[t] * r[s]\n",
    "\n",
    "        # Maximization step\n",
    "        # ML estimate for nu is just the frequency.\n",
    "        Z      = np.sum(c)\n",
    "        est_nu = np.divide(c, Z)\n",
    "\n",
    "        # We could make a fancy convergence test here.\n",
    "        # Running for 1000 iterations suffices, though it's overkill.\n",
    "        if iterations == 1000: converged = True\n",
    "    return est_nu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze our test data set by EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_nu  = expectation_maximization(r, L, T, S, seg_is_in)\n",
    "est_tau = to_tau(est_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM estimates:\n",
      "isoform       est_tau   true_tau\n",
      "Arc1            0.396      0.392\n",
      "Arc2            0.184      0.189\n",
      "Arc3            0.071      0.065\n",
      "Arc4            0.027      0.029\n",
      "Arc5            0.036      0.038\n",
      "Arc6            0.026      0.022\n",
      "Arc7            0.026      0.030\n",
      "Arc8            0.043      0.038\n",
      "Arc9            0.177      0.181\n",
      "Arc10           0.014      0.016\n",
      "\n",
      "logL, est  = -2036185.4\n",
      "logL, true = -2036190.9\n"
     ]
    }
   ],
   "source": [
    "print(\"EM estimates:\")\n",
    "print(\"{0:10s}  {1:>9s}  {2:>9s}\".format(\"isoform\", \"est_tau\", \"true_tau\"))\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:9.3f}  {2:9.3f}\".format(Tlabel[i], est_tau[i], true_tau[i]))\n",
    "print('')\n",
    "\n",
    "print(\"logL, est  = {0:.1f}\".format(loglikelihood(r, est_nu,  L, T, S, seg_is_in)))\n",
    "print(\"logL, true = {0:.1f}\".format(loglikelihood(r, true_nu, L, T, S, seg_is_in)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks ok. Our estimated $\\tau$ are almost identical to the known true $\\tau$ on our generated data: a positive control that our EM estimation is working properly.  Our log likelihood is guaranteed to be a global optimum in this problem, so another check that things are working as expected is that we obtain a log likelihood at least as good as the likelihood of the true $\\tau$, and we do. We do a little better because we're overfitting stochastic noise in the data to some negligible degree.\n",
    "\n",
    "We're ready to analyze Lestrade's supplementary data file now!\n",
    "\n",
    "We could parse the data file but it's easy enough to just plug the numbers in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The supplementary data from Lestrade et al.\n",
    "T = 10\n",
    "S = 10\n",
    "\n",
    "\n",
    "r = np.array([  89357,  69196, 87852, 99854, 57369,  85715, 197730, 213016, 61271, 38640 ]).astype(int)\n",
    "L = np.array([      4,      2,     3,     4,     4,      3,      2,      2,     3,     3 ]).astype(int)\n",
    "\n",
    "seg_is_in, segusage = make_auxiliary_arrays(L, T, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution that Lestrade got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_lestrade  = lestrade_estimate(r, L, T, S, segusage)\n",
    "tau_lestrade = to_tau(nu_lestrade, L)                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our EM solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_nu  = expectation_maximization(r, L, T, S, seg_is_in)\n",
    "est_tau = to_tau(est_nu, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isoform      Lestrade        ours\n",
      "Arc1            0.085       0.142\n",
      "Arc2            0.077       0.019\n",
      "Arc3            0.080       0.084\n",
      "Arc4            0.096       0.053\n",
      "Arc5            0.111       0.023\n",
      "Arc6            0.129       0.163\n",
      "Arc7            0.151       0.313\n",
      "Arc8            0.123       0.095\n",
      "Arc9            0.078       0.076\n",
      "Arc10           0.071       0.032\n",
      "\n",
      "logL, Lestrade's  = -2190089.6\n",
      "logL, ours        = -2165610.4\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:10s}  {1:>9s}   {2:>9s}\".format(\"isoform\", \"Lestrade\", \"ours\"))\n",
    "for i in range(T):\n",
    "    print (\"{0:10s}  {1:9.3f}   {2:9.3f}\".format(Tlabel[i], tau_lestrade[i], est_tau[i]))\n",
    "print('')\n",
    "\n",
    "print(\"logL, Lestrade's  = {0:.1f}\".format(loglikelihood(r, nu_lestrade, L, T, S, seg_is_in)))\n",
    "print(\"logL, ours        = {0:.1f}\".format(loglikelihood(r, est_nu,      L, T, S, seg_is_in)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**that's it...** \n",
    "When we analyze Lestrade's data, we find that their conclusion that all Arc isoforms are roughly equally expressed is wrong. Their analysis biased their conclusion, by assigning read counts uniformly to the possible isoforms they could have come from. Actually, Arc2, Arc5, and Arc10 are only weakly expressed, and Arc7 accounts for 30% of the RNA population; the difference between their expression levels is about 10-15x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
